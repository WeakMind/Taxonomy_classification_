# -*- coding: utf-8 -*-
"""Data Validation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fexeNRIwFn8hrGLJt6oNJI02EKUt3ZU4
"""

import pandas as pd
from IPython.display import display
from gensim.utils import simple_preprocess
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# test = pd.ExcelFile('./Testing_data_processed.xlsx')
# test_df = pd.read_excel(test)

test = pd.ExcelFile('/content/Dummy_Data_Testing.xlsx')
test_df = pd.read_excel(test)

a = 'gfhh'

str(a)[:2]=='gf'

test_df.shape

MDB_data = pd.ExcelFile('./MDB_Dataset_Final.xlsx')

sheet_data_list = []

for sheet_name in MDB_data.sheet_names:
  data = pd.read_excel(MDB_data,sheet_name)
  data.columns = [(x.lower().strip()) for x in data.columns]
  sheet_data_list.append(data)

combined_data = pd.concat(sheet_data_list,ignore_index=True)

combined_data.shape

combined_data.isna().sum()*100/combined_data.shape[0]

test_df['text'] = test_df['detailed description'].apply(lambda x: len(str(x).split(' ')))

test_df['text'].mean()

project_name_asset_data['text'] = project_name_asset_data['project_asset'].apply(lambda x: len(str(x).split()))

project_name_asset_data['text'].mean()

test_df.head(2)

combined_data.head(2)

combined_data.head()

combined_data.isna().sum()

combined_data.dtypes

combined_data.fillna('',inplace=True)

combined_data['val_sector'] = combined_data.apply(lambda x : 1 if 'Building' in x['sector'] else 0,axis=1)

combined_data['val_sector'].value_counts()

combined_data.columns

test_df.columns

string = ['Measures that reduce net energy consumption; resource consumption or CO2e emissions; or measures that increase plant-based carbon sinks in new or retrofitted buildings and associated grounds; enabling certification standards to be met',
          'Projects that support mining of minerals and metal ores prevalently used in or critical for renewable energy; technologies that increase energy efficiency; other low-carbon technologies; or materials and products with low embedded GHG emissions']
combined_data['activity']= combined_data.apply(lambda x : x['activity'].replace(';',',') if (x['activity'] in (string)) else x['activity'],axis=1)

str_old = ['Energy and resource efficiency and demand management in water supply']
str_cr =  ['Greenfield and brownfield projects that promote improved operation and maintenance to reduce water losses, promote energy savings, or meet or exceed wastewater treatment targets']
str_new = ['Energy and resource efficiency and GHG- emission reduction in water supply and wastewater management']
combined_data['category']= combined_data.apply(lambda x : x['category'].replace(str_old[0],str_new[0]) if ((x['category'] in (str_old)) and (x['activity'] in (str_cr))) else x['category'],axis=1)

act_old = ['Projects that contribute to reduction of GHG emissions through production of biomaterials/bioenergy from biomass']
act_new = ['Projects that support production of components, equipment or infrastructure dedicated exclusively to utilisation in the renewable energy, energy efficiency improvement, or other low-carbon technologies']
sec_cr = ['Manufacturing']
cat_cr = ['Support for low- carbon development']
combined_data['activity']= combined_data.apply(lambda x : x['activity'].replace(act_old[0],act_new[0]) if ((x['sector'] in (sec_cr)) and (x['category'] in (cat_cr))) else x['activity'],axis=1)

description_dict = {}
all_descriptions = set(combined_data['criteria'].tolist())
max_class = -1
for i,val in enumerate(all_descriptions):
  description_dict[val] = i
  max_class = max(max_class,i)


def assign_description(row):
  if row['criteria'] in description_dict.keys():
    return description_dict[row['criteria']]
  else:
    return max_class


combined_data['criteria_class'] = combined_data.apply(lambda row : assign_description(row), axis=1)

combined_data['criteria_class'].value_counts()

input_cols = ['industry','project name','asset type', 'short description', 'type',
       'detailed description','final assessment cbi','criteria_class','criteria','activity']

output_cols = ['sector','category','activity','criteria']

combined_data.shape

combined_data_final = combined_data

combined_data_final.shape

input_data = combined_data_final[input_cols]

test_df.columns

# import pandas as pd
# from nltk.corpus import stopwords
# from nltk.stem import PorterStemmer
# from nltk.stem import WordNetLemmatizer
# import re
# import nltk
# nltk.download('wordnet')
# nltk.download('stopwords')

# # Sample DataFrame
# # data = {'text': ["running", "flies", "better", "studies"]}
# # df = pd.DataFrame(data)

# # Initialize the lemmatizer and stemmer
# lemmatizer = WordNetLemmatizer()
# stemmer = PorterStemmer()

# # Define a function for cleaning and processing text
# def clean_and_process_text(text):
#     # Convert text to lowercase
#     text = text.lower()

#     # Remove special characters, numbers, and punctuation
#     text = re.sub(r'[^a-zA-Z\s]', '', text)

#     # Tokenize the text
#     words = text.split()

#     # Remove stop words
#     stop_words = set(stopwords.words('english'))
#     words = [word for word in words if word not in stop_words]

#     # Apply lemmatization to get the base form of words
#     lemmatized_words = [lemmatizer.lemmatize(word) for word in words]

#     # Apply stemming to get word stems
#     stemmed_words = [stemmer.stem(word) for word in words]

#     # Join the cleaned, lemmatized, and stemmed words back into a single string
#     cleaned_text = ' '.join(stemmed_words)

#     return cleaned_text

# # Apply the cleaning function to the 'text' column in the DataFrame
# #df['cleaned_text'] = df['text'].apply(clean_and_process_text)

# # Display the cleaned and processed text
# #print(df)

####------TEST DATA PREPROCESS-----######
####------For 348 Projects---------######


test_df.columns = ['Sector_input', 'Category_input', 'Detailed description', 'Sector', 'Category',
       'Activity', 'Criteria', 'Final Assessment CBI']
processed_test = test_df
processed_test['project_desc'] = processed_test.apply(lambda row : str(row['Sector_input']) + ' '+ str(row['Detailed description']) ,axis=1)
processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(lambda x: ' '.join(simple_preprocess(x)))
#processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(clean_and_process_text)

####------TEST DATA PREPROCESS-----######
####------For Dummy Data---------######


test_df.columns = ['Project Number', 'Industry', 'Project Name', 'Asset Type','Short description',
       'Detailed description','Type']
processed_test = test_df
processed_test['project_desc'] = processed_test.apply(lambda row : str(row['Industry']) +' '+ str(row['Asset Type'])+ ' ' + str(row['Short description']) ,axis=1)
processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(lambda x: ' '.join(simple_preprocess(x)))
#processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(clean_and_process_text)

####------TRAINING DATA PREPROCESS-----######

project_name_asset_data = input_data
project_name_asset_data.fillna("",inplace=True)

# with short and detailed description
project_name_asset_data['project_asset'] = project_name_asset_data.apply(lambda row : str(row['industry'])+' '+str(row['project name']) + ' '+ str(row['asset type']) + ' ' + str(row['short description'])+ ' ' + str(row['detailed description']),axis=1)
project_name_asset_data['processed_project_asset'] = project_name_asset_data.loc[:, 'project_asset'].apply(lambda x: ' '.join(simple_preprocess(x)))
# project_name_asset_data['processed_project_asset'] = project_name_asset_data.loc[:, 'project_asset'].apply(clean_and_process_text)

project_name_asset_data.shape

project_name_asset_data.columns

project_name_asset_data.head(2)

processed_test[processed_test['Detailed description'].notnull()].tail(30)

# pip install pandas scikit-learn tensorflow

# pip install keras

# #########------Neural Nets(doesn't work)-------#######

# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import LabelEncoder
# from sklearn.feature_extraction.text import TfidfVectorizer
# import numpy as np
# import tensorflow as tf
# from tensorflow import keras
# from tensorflow.keras.preprocessing.text import Tokenizer
# from tensorflow.keras.preprocessing.sequence import pad_sequences

# # Load and preprocess your data
# # Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# # Split the data into training and testing sets
# # X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)
# X_train = project_name_asset_data['processed_project_asset']
# y_train = project_name_asset_data['criteria'].to_numpy()

# X_test = processed_test['processed_project_desc']
# # y_test = processed_test['Criteria'].to_numpy()


# # Create a label encoder
# label_encoder = LabelEncoder()
# y_train = label_encoder.fit_transform(y_train)
# # y_test = label_encoder.transform(y_test)

# # Tokenize the text data
# max_words = 10000  # Adjust the number of words as needed
# tokenizer = Tokenizer(num_words=max_words)
# tokenizer.fit_on_texts(X_train)
# X_train_seq = tokenizer.texts_to_sequences(X_train)
# X_test_seq = tokenizer.texts_to_sequences(X_test)

# # Pad the sequences to a fixed length
# max_sequence_length = 100  # Adjust the sequence length as needed
# X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)
# X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length)

# # Build a simple neural network
# model = keras.Sequential()
# model.add(keras.layers.Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))
# model.add(keras.layers.Flatten())
# model.add(keras.layers.Dense(64, activation='relu'))
# model.add(keras.layers.Dense(1, activation='softmax'))#here it should be softmax

# # Compile the model
# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])#get loss for multiclass classification in keras not 'binary_crossentropy'

# # Train the model
# model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_split=0.2)

# y_pred = model.predict(X_test_pad)
# predicted_labels = np.argmax(y_pred, axis=1)
# print('6')
# y_pred = label_encoder.inverse_transform(predicted_labels.astype(int))
# processed_test['Prediction'] = y_pred
# # Evaluate the model on the test data
# # loss, accuracy = model.evaluate(X_test_pad, y_test)
# # print(f"Test Accuracy: {accuracy * 100:.2f}%")

project_name_asset_data.head()

project_name_asset_data[project_name_asset_data['Sector']=='Building'].shape

project_name_asset_data[project_name_asset_data['criteria']==''].shape



len(weights)

len(classes_weights)

project_name_asset_data['processed_project_asset'].shape

project_name_asset_data.drop_duplicates(ignore_index=True).shape

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)
project_name_asset_data = project_name_asset_data.drop_duplicates(subset=['processed_project_asset'],ignore_index=True)
project_name_asset_data_ = project_name_asset_data[project_name_asset_data['activity']!='']
project_name_asset_data__ = project_name_asset_data[project_name_asset_data['activity']==''].head(300)
project_name_asset_data = pd.concat([project_name_asset_data_,project_name_asset_data__])


# Determine the target number of samples per class (e.g., the number of samples in the minority class)
# target_samples_per_class = project_name_asset_data['description'].value_counts().max()
# target_samples_per_class = 300

# Initialize an empty DataFrame for the balanced dataset
# balanced_df = pd.DataFrame(columns=project_name_asset_data.columns)

# # Oversample minority classes and undersample the majority class
# for class_label in project_name_asset_data['description'].unique():
#     class_data = project_name_asset_data[project_name_asset_data['description'] == class_label]
#     if len(class_data) < target_samples_per_class:
#         # Oversample by duplicating samples
#         num_duplicates = target_samples_per_class // len(class_data)
#         oversampled_class_data = pd.concat([class_data] * num_duplicates, ignore_index=True)
#         balanced_df = pd.concat([balanced_df, oversampled_class_data])
#     else:
#         # Undersample by using all samples
#         balanced_df = pd.concat([balanced_df, class_data.sample(target_samples_per_class)])

# print(balanced_df)

# project_name_asset_data = balanced_df

X_train = project_name_asset_data['processed_project_asset']
y_train = project_name_asset_data['activity']

X_test = processed_test['processed_project_desc']
#y_test_a = processed_test['Criteria'].to_numpy()

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=10000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
from sklearn.utils import class_weight
classes_weights = list(class_weight.compute_class_weight(class_weight = 'balanced',
                                             classes = np.unique(project_name_asset_data['activity']),
                                             y = project_name_asset_data['activity']))

weights = np.ones(y_train.shape[0], dtype = 'float')
for i, val in enumerate(y_train):
    weights[i] = classes_weights[val-1]

xgb_classifier = xgb.XGBClassifier(sample_weight=weights)
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
y_pred_proba = xgb_classifier.predict_proba(X_test_tfidf)
max_class_probabilities = np.max(y_pred_proba, axis=1)

print('6')
y_pred_transform = le.inverse_transform(y_pred)
len(y_pred_transform)
print('7')

processed_test['Prediction'] = y_pred_transform
processed_test['Probability'] = max_class_probabilities
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

processed_test['Prediction'] = processed_test['Prediction'].replace('\n','', regex=True)

processed_test['Prediction'] = processed_test['Prediction'].str.replace(r'\s*([,.])\s*', r'\1', regex=True)

Tax_Guide = pd.ExcelFile('/content/Taxonomies_and_principles.xlsx')

Tax_Guide.sheet_names

EU_guide = pd.read_excel(Tax_Guide, 'MDB Principles')

EU_guide.head()

processed_test['Prediction'] = processed_test['Prediction'].str.strip()
EU_guide['Activity'] = EU_guide['Activity'].str.strip()

EU_guide['Activity'] = EU_guide['Activity'].str.replace(r'\s*([,.])\s*', r'\1', regex=True)

EU_guide.head(1)

EU_guide['combined'] = EU_guide.apply(lambda x : str(x['Sector']) + " _ " + str(x['Category']) + " _ "+str(x['Activity']) \
                          + " _ "+str(x['Criteria']),axis=1)

EU_guide['combined'].head()

combined_data.head(1)

combined_data['processed_combined'] = combined_data.apply(lambda x : str(x['industry']) + " " +str(x['project name']) + " " + str(x['asset type']) + " "+str(x['short description']) \
                          + " "+str(x['detailed description']),axis=1)

all_activities = EU_guide['combined'].tolist()

!pip install -U sentence_transformers

from sentence_transformers import SentenceTransformer, util

combined_data_dedup = combined_data.drop_duplicates(subset=['processed_combined'],ignore_index=True)

combined_data_dedup = combined_data_dedup.head(30)

combined_data_dedup = test_df

model = SentenceTransformer('all-MiniLM-L6-v2')
def get_answer(row,all_activities):

  input_sentence = row
  embedding1 = model.encode(input_sentence, convert_to_tensor=True)
  vijeta = -1
  activity = ''
  scores = []
  for sentence in all_activities:
    possible_match = sentence
    embedding2 = model.encode(possible_match, convert_to_tensor=True)
    cosine_similarity = util.cos_sim(embedding1, embedding2)
    scores.append(cosine_similarity)
    if vijeta < cosine_similarity:
      vijeta = cosine_similarity
      activity = possible_match
  #print(scores)
  print(vijeta)
  print(activity.split(' _ ')[-2])
  return pd.Series([activity.split(' _ ')[-2],vijeta.cpu().numpy()[0][0]])

combined_data_dedup[['mini_LM_prediction','cosine_sim']] = combined_data_dedup.apply(lambda x : get_answer(x['processed_project_desc'],all_activities),axis=1)

combined_data_dedup['ensemble_prediction'] = combined_data_dedup.apply(lambda x : x['Prediction'] if x['Probability']>0.75 \
                                                                      else (x['mini_LM_prediction'] if x['cosine_sim']>0.35 else ''),axis=1)

combined_data_dedup.shape

combined_data_dedup











import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)
project_name_asset_data = project_name_asset_data.drop_duplicates(subset=['processed_project_asset'],ignore_index=True)
project_name_asset_data_ = project_name_asset_data[project_name_asset_data['activity']!='']
project_name_asset_data__ = project_name_asset_data[project_name_asset_data['activity']==''].head(600)
project_name_asset_data = pd.concat([project_name_asset_data_,project_name_asset_data__])

X_train = project_name_asset_data['processed_project_asset']
y_train = project_name_asset_data['activity']

X_test = processed_test['processed_project_desc']
#y_test_a = processed_test['Criteria'].to_numpy()

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=10000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
from sklearn.utils import class_weight
classes_weights = list(class_weight.compute_class_weight(class_weight = 'balanced',
                                             classes = np.unique(project_name_asset_data['activity']),
                                             y = project_name_asset_data['activity']))

weights = np.ones(y_train.shape[0], dtype = 'float')
for i, val in enumerate(y_train):
    weights[i] = classes_weights[val-1]

xgb_classifier = xgb.XGBClassifier(sample_weight=weights)
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
print('6')
y_pred_transform = le.inverse_transform(y_pred)
len(y_pred_transform)
print('7')

processed_test['Prediction'] = y_pred_transform
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

xgb_classifier.get_xgb_params()

processed_test.head(30)['Prediction'].values

processed_test.head(30)

Tax_Guide = pd.ExcelFile('/content/Taxonomies_and_principles.xlsx')

Tax_Guide.sheet_names

MDB_guide = pd.read_excel(Tax_Guide, 'MDB Principles')

MDB_guide.head()

MDB_guide['combined'] = MDB_guide.apply(lambda x : str(x['Sector']) + " _ " + str(x['Category']) + " _ "+str(x['Activity']) \
                          + " _ "+str(x['Criteria']),axis=1)

MDB_guide['combined'].head()

combined_data['processed_combined'] = combined_data.apply(lambda x : str(x['industry']) + " " +str(x['project name']) + " " + str(x['asset type']) + " "+str(x['short description']) \
                          + " "+str(x['detailed description']),axis=1)

all_activities = MDB_guide['combined'].tolist()

!pip install -U sentence_transformers

test_df.head()

from sentence_transformers import SentenceTransformer, util

combined_data_dedup = combined_data.drop_duplicates(subset=['processed_combined'],ignore_index=True)

combined_data_dedup = combined_data_dedup.head(30)

combined_data_dedup = test_df

model = SentenceTransformer('all-MiniLM-L6-v2')
def get_answer(row,all_activities):

  input_sentence = row
  embedding1 = model.encode(input_sentence, convert_to_tensor=True)
  vijeta = -1
  activity = ''
  scores = []
  for sentence in all_activities:
    possible_match = sentence
    embedding2 = model.encode(possible_match, convert_to_tensor=True)
    cosine_similarity = util.cos_sim(embedding1, embedding2)
    scores.append(cosine_similarity)
    if vijeta < cosine_similarity:
      vijeta = cosine_similarity
      activity = possible_match
  #print(scores)
  print(vijeta)
  print(activity.split(' _ ')[-2])
  return activity.split(' _ ')[-2]


combined_data_dedup['prediction'] = combined_data_dedup.apply(lambda x : get_answer(x['processed_project_desc'],all_activities),axis=1)

print(combined_data_dedup[combined_data_dedup['activity']==combined_data_dedup['prediction']].shape)

combined_data_dedup[['processed_combined','activity','prediction']].head(20)

string = ['Generation of renewable energy with low lifecycle GHG emissions to supply electricity, heating, mechanical energy or cooling']
MDB_guide[MDB_guide['Activity'].isin(string)].head(2)

MDB_guide['Activity'] = MDB_guide['Activity'].astype('str')

string = ['•\tWhere the very-low-carbon electricity is renewable energy, it shall meet the same criteria as in Energy sector activity: Generation of renewable energy with low lifecycle GHG emissions to supply electricity, heating, mechanical energy or cooling and activity: Joint use of renewable energy and fossil fuel to supply electricity, heat, mechanical energy or cooling.\n•\tA transmission or distribution project dedicated to the evacuation of only very-low-carbon electricity shall be fully eligible, with the exception of dedicated evacuation of new nuclear power generation.\n•\tA transmission or distribution project dedicated to the evacuation of electricity from plants that do not generate very-low-carbon electricity shall not be eligible.\n•\tIn all other transmission or distribution projects, the entity applying the Common Principles shall demonstrate that the electricity system is increasing the share of non-nuclear very-low-carbon electricity use, or if the system is already dominated by very-low-carbon electricity and there is little scope for expanding the share of non-nuclear very-low-carbon electricity, the activity shall not decrease the current share.\n•\tIf the activity involves an interconnection between electricity systems, the entity applying the Common Principles shall demonstrate that the investment will not significantly increase GHG emissions over the short or medium term.\n•\tThe increasing share of non-nuclear very-low-carbon electricity shall be reflected in the most recent power system development plan covering a planning horizon of up to 10 years and the financing shall be apportioned using the projected share of very-low-carbon electricity in the electricity being transported in the entire electricity system in which the activity will be undertaken at the end of the planning horizon. If the system planning horizon does not extend to 10 years but there is an officially recognised decarbonisation plan for the electricity system that extends up to 10 years, and the decarbonisation plan is consistent with the power system development plan, then the financing can be apportioned using the projected share of very-low- carbon electricity in the electricity system at the end of 10 years in the decarbonisation plan.','•\tThe entity applying the Common Principles shall demonstrate a substantially lower carbon intensity or energy intensity of the greenfield manufacturing facility or greenfield supplementary equipment or production lines at an existing manufacturing facility against a selected benchmark.\n•\tThe financing provided for a greenfield facility shall be apportioned according to the share of the total finance devoted to enabling high efficiency in a manner consistent with the principles of conservativeness and granularity.\n•\tComponents of activities that use fossil fuels shall not be eligible.']
MDB_guide[MDB_guide['Criteria'].isin(string)].head(2)

# combined_data_refined = combined_data[['sector', 'category',
#        'activity', 'criteria','criteria_class']]
# temp_cols = ['sector', 'category',
#        'activity', 'criteria']

# for column in temp_cols:
#   combined_data_refined[column] = combined_data_refined.apply(lambda x : '' if pd.isna(x['criteria']) else x[column],axis=1)


# # combined_data_refined['sector'] = combined_data_refined.apply(lambda x : '' if pd.isna(x['description']) else x['sector'],axis=1)
# # combined_data_refined['activity'] = combined_data_refined.apply(lambda x : '' if pd.isna(x['description']) else x['activity'],axis=1)

# combined_data_ground = combined_data_refined.copy()
# combined_data_predicted = combined_data_refined.copy()

combined_data_predicted = MDB_guide.copy()
ground_truth_cols = []
predicted_cols = []
for i,col in enumerate(MDB_guide.columns):
#  ground_truth_cols.append(col+'_GroundTruth')
  predicted_cols.append(col+'_Predicted')

#combined_data_ground.columns = ground_truth_cols
combined_data_predicted.columns = predicted_cols

combined_data_predicted.shape

combined_data_predicted.columns

combined_data_predicted.head()

type(combined_data_predicted['Activity_Predicted'][0])

#combined_data_ground = combined_data_ground.drop_duplicates()
combined_data_predicted =combined_data_predicted.drop_duplicates()

combined_data_predicted.columns

combined_data_predicted.shape

All_predictions_df = processed_test.copy()

#All_predictions_df['criteria_class_prediction'] = All_predictions_df.apply(lambda x : x['Prediction'].split('__')[-1],axis=1)
All_predictions_df['activity_prediction'] = All_predictions_df['Prediction'].astype('str')
combined_data_predicted['Activity_Predicted'] = combined_data_predicted['Activity_Predicted'].astype('str')

#All_predictions_df['criteria_class_groundtruth'] = All_predictions_df.apply(lambda x : x['Label'].split('__')[-1],axis=1)
#All_predictions_df['criteria_class_groundtruth'] = All_predictions_df['criteria_class'].astype('int')

All_predictions_df.head(2)

combined_data_predicted.head(2)

combined_data_predicted.shape

All_predictions_df.shape

#####----for dummy data------######

#All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='criteria_class_groundtruth',right_on='criteria_class_GroundTruth',how='left')
All_predictions_df_joined = All_predictions_df.merge(combined_data_predicted,left_on='activity_prediction',right_on='Activity_Predicted',how='left')

#All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='criteria_class_groundtruth',right_on='criteria_class_GroundTruth',how='left')
All_predictions_df_joined = All_predictions_df.merge(combined_data_predicted,left_on='criteria_prediction',right_on='Criteria_Predicted',how='left')

All_predictions_df_joined.shape

All_predictions_df_joined.head(30)

display(All_predictions_df_joined[All_predictions_df_joined['Sector_Predicted'].notnull()])

All_predictions_df_joined[['Sector','Sector_Predicted']].value_counts()

All_predictions_df_joined[All_predictions_df_joined['Sector_Predicted'].notnull()].shape

display(print(All_predictions_df_joined[['Activity','Activity_Predicted']].value_counts()))

All_predictions_df_joined.fillna('',inplace=True)

display(All_predictions_df_joined[All_predictions_df_joined['Sector']!='Buildings'].head(30))







correct_string = 'Potentially aligned if compliant with screening indicator'
variations_to_replace = ['Compatible if compliant with screening indicator or if more information is found',
                         'Aligned if compliant with screening indicator',
                         'Compatible if compliant with screening indicator or if more information is found \n']

def replace_variations(text):
    if text in variations_to_replace:
        return correct_string
    else:
        return text

combined_data_final['final assessment cbi'] = combined_data_final['final assessment cbi'].apply(replace_variations)

combined_data_final['final assessment cbi'].replace('Automatically aligned\n','Automatically aligned',inplace= True)
combined_data_final['final assessment cbi'].replace('Not Compatible (not aligned)','Not aligned',inplace= True)
combined_data_final['final assessment cbi'].replace('Not Compatible ','Not aligned',inplace= True)
combined_data_final['final assessment cbi'].replace('Not Compatible','Not aligned',inplace= True)
combined_data_final['final assessment cbi'].replace('Insufficient guidance available* (criteria not developed)',
                      'Insufficient guidance available (criteria not developed)',inplace= True)
combined_data_final['final assessment cbi'].replace('Insufficient detail available to determine compatibility',
                      'Insufficient detail available to determine alignment',inplace= True)

combined_data_final['final assessment cbi'].unique()

All_predictions_df_joined.head(30)

All_predictions_df_joined['Final Assessment CBI'].replace('Insufficient detail available to determaine alignment',
                      'Insufficient detail available to determine alignment',inplace= True)
All_predictions_df_joined['Final Assessment CBI'].replace('Aligned if compliant with screening indicator',
                      'Potentially aligned if compliant with screening indicator',inplace= True)



All_predictions_df_joined.columns

All_predictions_df_joined.shape

All_predictions_df_joined['Final Assessment CBI'].unique()

####------TEST DATA PREPROCESS-----######

final_test = All_predictions_df_joined.copy()
final_test['sec_cat_act_cri'] = final_test.apply(lambda row : str(row['Project Name']) +  ' '+ str(row['Industry']) +  ' ' + str(row['Asset Type']) + ' ' + str(row['Short description']) + ' '+ str(row['Detailed description']) ,axis=1)
final_test['processed_sec_cat_act_cri'] = final_test.loc[:, 'sec_cat_act_cri'].apply(lambda x: ' '.join(simple_preprocess(x)))
#final_test['processed_sec_cat_act_cri'] = final_test.loc[:, 'sec_cat_act_cri'].apply(clean_and_process_text)

combined_data_final.columns

####------TRAINING DATA PREPROCESS-----######

final_train = combined_data_final.copy()
final_train.fillna("",inplace=True)
final_train['descrpt'] = final_train.apply(lambda row : str(row['project name']) +  ' '+ str(row['industry']) +  ' ' + str(row['asset type']) + ' ' + str(row['short description']) + ' '+ str(row['detailed description']) ,axis=1)
final_test['processed_sec_cat_act_cri'] = final_test.loc[:, 'sec_cat_act_cri'].apply(lambda x: ' '.join(simple_preprocess(x)))
final_train['processed_descrpt'] = final_train.loc[:, 'descrpt'].apply(lambda x: ' '.join(simple_preprocess(x)))
# final_train['processed_descrpt'] = final_train.loc[:, 'descrpt'].apply(clean_and_process_text)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)
project_name_asset_data = project_name_asset_data.drop_duplicates(subset=['processed_project_asset'],ignore_index=True)
project_name_asset_data_ = project_name_asset_data[project_name_asset_data['des']!='']
project_name_asset_data__ = project_name_asset_data[project_name_asset_data['activity']==''].head(600)
project_name_asset_data = pd.concat([project_name_asset_data_,project_name_asset_data__])

X_train = project_name_asset_data['processed_project_asset']
y_train = project_name_asset_data['activity']

X_test = processed_test['processed_project_desc']
#y_test_a = processed_test['Criteria'].to_numpy()

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=10000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
from sklearn.utils import class_weight
classes_weights = list(class_weight.compute_class_weight(class_weight = 'balanced',
                                             classes = np.unique(project_name_asset_data['activity']),
                                             y = project_name_asset_data['activity']))

weights = np.ones(y_train.shape[0], dtype = 'float')
for i, val in enumerate(y_train):
    weights[i] = classes_weights[val-1]

xgb_classifier = xgb.XGBClassifier(sample_weight=weights)
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
print('6')
y_pred_transform = le.inverse_transform(y_pred)
len(y_pred_transform)
print('7')

processed_test['Prediction'] = y_pred_transform
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)

X_train = final_train['processed_descrpt']
y_train = final_train['final assessment cbi']

X_test = final_test['processed_sec_cat_act_cri']
# y_test = final_test['Final Assessment CBI']

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
xgb_classifier = xgb.XGBClassifier()
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
print('6')
y_pred = le.inverse_transform(y_pred)
len(y_pred)
print('7')

final_test['Final_Assessment_Prediction'] = y_pred
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

final_test.head(2)

final_test[final_test['Final Assessment CBI']==final_test['Final_Assessment_Prediction']].shape[0]/final_test.shape[0]

print(final_test[['Final_Assessment_Prediction']].value_counts())



test_df[test_df['Final Assessment CBI']=='Insufficient detail available to determaine alignment'].head(50)

test_df['Final Assessment CBI'].value_counts()

combined_data['final assessment cbi'].value_counts()

combined_data.columns

combined_data['temp'] = combined_data['industry'].apply(lambda x : 1 if 'agro' in x else 0)

combined_data[combined_data['temp']==1].head()

final_test.to_excel('./Test_Prediction_Output_Dummy.xlsx')





import pandas as pd
import numpy as np
from IPython.display import display
from gensim.utils import simple_preprocess
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

#######---------EU Validation------########

test = pd.ExcelFile('/content/Dummy_Data_Testing.xlsx')
test_df = pd.read_excel(test)

EU_data = pd.ExcelFile('./EU_Dataset_Final.xlsx')

sheet_data_list = []

for sheet_name in EU_data.sheet_names:
  data = pd.read_excel(EU_data,sheet_name)
  data.columns = [(x.lower().strip()) for x in data.columns]
  sheet_data_list.append(data)

combined_data = pd.concat(sheet_data_list,ignore_index=True)

combined_data.head(1)

description_dict = {}
all_descriptions = set(combined_data['description'].tolist())
max_class = -1
for i,val in enumerate(all_descriptions):
  description_dict[val] = i
  max_class = max(max_class,i)

def assign_description(row):
  if row['description'] in description_dict.keys():
    return description_dict[row['description']]
  else:
    return max_class

input_cols = ['industry','project name','asset type', 'short description', 'type',
       'detailed description','description_class','description','final assessment cbi comments']

output_cols = ['sector','activity','description']

combined_data['description_class'] = combined_data.apply(lambda row : assign_description(row), axis=1)

combined_data.head(2)

combined_data.shape

# combined_data.fillna('',inplace=True)

combined_data_final = combined_data

combined_data_final.fillna('',inplace=True)

combined_data_final.shape

combined_data_final['final assessment cbi comments'].replace('Compatible if compliant with screening indicator or if more information is found \n',
                      'Compatible if compliant with screening indicator or if more information is found',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Compatible if compliant with screening indicator or if more information is found',
                      'Potentially aligned if compliant with screening indicator',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Insufficient detail available to determine compatibility',
                      'Insufficient detail available to determine alignment',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Insufficient guidance available (criteria not developed)',
                      'Insufficient guidance available (criteria not developed)',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Insufficient detail available to determine compatibility',
                      'Insufficient detail available to determine alignment',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Not Compatible ','Not aligned',inplace= True)

combined_data_final['final assessment cbi comments'].unique()

combined_data_final.shape

input_data = combined_data_final[input_cols]

output = combined_data_final[output_cols]



####------TEST DATA PREPROCESS-----######
####------For 348 Projects---------######


test_df.columns = ['Sector_input', 'Category_input', 'Detailed description', 'Sector', 'Category',
       'Activity', 'Criteria', 'Final Assessment CBI']
processed_test = test_df
processed_test['project_desc'] = processed_test.apply(lambda row : str(row['Sector_input']) + ' '+ str(row['Detailed description']) ,axis=1)
processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(lambda x: ' '.join(simple_preprocess(x)))
#processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(clean_and_process_text)

####------TEST DATA PREPROCESS-----######
####------For Dummy Data---------######


test_df.columns = ['Project Number', 'Industry', 'Project Name', 'Asset Type','Short description',
       'Detailed description','Type']
processed_test = test_df
processed_test['project_desc'] = processed_test.apply(lambda row : str(row['Industry']) +' '+ str(row['Asset Type'])+ ' ' + str(row['Short description']) ,axis=1)
processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(lambda x: ' '.join(simple_preprocess(x)))
#processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(clean_and_process_text)

input_data.head(2)

####------TRAINING DATA PREPROCESS-----######

project_name_asset_data = input_data
project_name_asset_data.fillna("",inplace=True)

# with short and detailed description
project_name_asset_data['project_asset'] = project_name_asset_data.apply(lambda row : str(row['industry'])+' '+str(row['project name']) + ' '+ str(row['asset type']) + ' ' + str(row['short description'])+ ' ' + str(row['detailed description']),axis=1)
project_name_asset_data['processed_project_asset'] = project_name_asset_data.loc[:, 'project_asset'].apply(lambda x: ' '.join(simple_preprocess(x)))
# project_name_asset_data['processed_project_asset'] = project_name_asset_data.loc[:, 'project_asset'].apply(clean_and_process_text)

project_name_asset_data = project_name_asset_data.drop_duplicates(subset=['processed_project_asset'],ignore_index=True)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)
project_name_asset_data = project_name_asset_data.drop_duplicates(subset=['processed_project_asset'],ignore_index=True)
project_name_asset_data_ = project_name_asset_data[project_name_asset_data['description']!='']
project_name_asset_data__ = project_name_asset_data[project_name_asset_data['description']==''].head(900)
project_name_asset_data = pd.concat([project_name_asset_data_,project_name_asset_data__])


# Determine the target number of samples per class (e.g., the number of samples in the minority class)
# target_samples_per_class = project_name_asset_data['description'].value_counts().max()
# target_samples_per_class = 300

# Initialize an empty DataFrame for the balanced dataset
# balanced_df = pd.DataFrame(columns=project_name_asset_data.columns)

# # Oversample minority classes and undersample the majority class
# for class_label in project_name_asset_data['description'].unique():
#     class_data = project_name_asset_data[project_name_asset_data['description'] == class_label]
#     if len(class_data) < target_samples_per_class:
#         # Oversample by duplicating samples
#         num_duplicates = target_samples_per_class // len(class_data)
#         oversampled_class_data = pd.concat([class_data] * num_duplicates, ignore_index=True)
#         balanced_df = pd.concat([balanced_df, oversampled_class_data])
#     else:
#         # Undersample by using all samples
#         balanced_df = pd.concat([balanced_df, class_data.sample(target_samples_per_class)])

# print(balanced_df)

# project_name_asset_data = balanced_df

X_train = project_name_asset_data['processed_project_asset']
y_train = project_name_asset_data['description']

X_test = processed_test['processed_project_desc']
#y_test_a = processed_test['Criteria'].to_numpy()

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=10000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
from sklearn.utils import class_weight
classes_weights = list(class_weight.compute_class_weight(class_weight = 'balanced',
                                             classes = np.unique(project_name_asset_data['description']),
                                             y = project_name_asset_data['description']))

weights = np.ones(y_train.shape[0], dtype = 'float')
for i, val in enumerate(y_train):
    weights[i] = classes_weights[val-1]

xgb_classifier = xgb.XGBClassifier(sample_weight=weights)
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
y_pred_proba = xgb_classifier.predict_proba(X_test_tfidf)
max_class_probabilities = np.max(y_pred_proba, axis=1)

print('6')
y_pred_transform = le.inverse_transform(y_pred)
len(y_pred_transform)
print('7')

processed_test['Prediction'] = y_pred_transform
processed_test['Probability'] = max_class_probabilities
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

processed_test.head(30)

processed_test['Prediction'] = processed_test['Prediction'].replace('\n','', regex=True)

processed_test['Prediction'] = processed_test['Prediction'].str.replace(r'\s*([,.])\s*', r'\1', regex=True)

Tax_Guide = pd.ExcelFile('/content/Taxonomies_and_principles.xlsx')

EU_guide = pd.read_excel(Tax_Guide, 'EU taxonomy')

processed_test['Prediction'] = processed_test['Prediction'].str.strip()
EU_guide['Description'] = EU_guide['Description'].str.strip()

EU_guide['Description'] = EU_guide['Description'].str.replace(r'\s*([,.])\s*', r'\1', regex=True)

EU_guide.head(1)

EU_guide['combined'] = EU_guide.apply(lambda x : str(x['Sector']) + " _ "+str(x['Activity']) \
                          + " _ "+str(x['Description']),axis=1)

EU_guide['combined'].head()

combined_data['processed_combined'] = combined_data.apply(lambda x : str(x['industry']) + " " +str(x['project name']) + " " + str(x['asset type']) + " "+str(x['short description']) \
                          + " "+str(x['detailed description']),axis=1)

all_activities = EU_guide['combined'].tolist()

!pip install -U sentence_transformers

from sentence_transformers import SentenceTransformer, util

combined_data_dedup = combined_data.drop_duplicates(subset=['processed_combined'],ignore_index=True)

combined_data_dedup = combined_data_dedup.head(30)

combined_data_dedup = test_df

model = SentenceTransformer('all-MiniLM-L6-v2')
def get_answer(row,all_activities):

  input_sentence = row
  embedding1 = model.encode(input_sentence, convert_to_tensor=True)
  vijeta = -1
  activity = ''
  scores = []
  for sentence in all_activities:
    possible_match = sentence
    embedding2 = model.encode(possible_match, convert_to_tensor=True)
    cosine_similarity = util.cos_sim(embedding1, embedding2)
    scores.append(cosine_similarity)
    if vijeta < cosine_similarity:
      vijeta = cosine_similarity
      activity = possible_match
  #print(scores)
  print(vijeta)
  print(activity.split(' _ ')[-2])
  return pd.Series([activity.split(' _ ')[-2],vijeta.cpu().numpy()[0][0]])

combined_data_dedup[['mini_LM_prediction','cosine_sim']] = combined_data_dedup.apply(lambda x : get_answer(x['processed_project_desc'],all_activities),axis=1)

combined_data_dedup.head()

combined_data_dedup['ensemble_prediction'] = combined_data_dedup.apply(lambda x : x['Prediction'] if x['Probability']>0.8 \
                                                                      else (x['mini_LM_prediction'] if x['cosine_sim']>0.4 else ''),axis=1)

combined_data_dedup.shape

combined_data_dedup

combined_data_dedup[['Project Number','ensemble_prediction']]

# from transformers import AutoModel, AutoTokenizer
# from sklearn.metrics.pairwise import cosine_similarity
# import torch

# # Check if GPU is available
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# # Load the ClimateBERT model and tokenizer
# model_name = "climatebert/distilroberta-base-climate-f"
# model = AutoModel.from_pretrained(model_name).to(device)
# tokenizer = AutoTokenizer.from_pretrained(model_name)


# combined_data_dedup = combined_data.drop_duplicates(subset=['processed_combined'],ignore_index=True)

# combined_data_dedup = combined_data_dedup.head(30)

# combined_data_dedup = test_df

# def get_answer(row,all_activities):

#   input_sentence = row
#   #embedding1 = model.encode(input_sentence, convert_to_tensor=True)
#   vijeta = -1
#   activity = ''
#   scores = []
#   for sentence in all_activities:
#     possible_match = sentence
#     # Tokenize and encode the sentences
#     inputs = tokenizer([input_sentence, possible_match], return_tensors="pt", padding=True, truncation=True).to(device)
#     # Get the embeddings for each sentence
#     with torch.no_grad():
#       embeddings = model(**inputs).last_hidden_state
#     # Calculate cosine similarity between the sentence embeddings
#     similarity = cosine_similarity(embeddings[0].cpu(), embeddings[1].cpu())
#     similarity = similarity[0][0]  # Extract the similarity score
#     scores.append(similarity)
#     if vijeta < similarity:
#       vijeta = similarity
#       activity = possible_match
#   #print(scores)
#   print(row)
#   print(vijeta)
#   print(activity.split(' _ ')[-1])
#   print('\n\n')
#   return activity.split(' _ ')[-1]


# combined_data_dedup['prediction'] = combined_data_dedup.apply(lambda x : get_answer(x['processed_project_desc'],all_activities),axis=1)

# # print(combined_data_dedup[combined_data_dedup['activity']==combined_data_dedup['prediction']].shape)

# import torch
# from transformers import AutoModel, AutoTokenizer
# import numpy as np
# from sklearn.metrics.pairwise import cosine_similarity

# # Load ClimateBERT model (or a similar pre-trained model)
# model_name = "impactlab/climatebert"  # Replace with the actual model name
# model = AutoModel.from_pretrained(model_name)
# tokenizer = AutoTokenizer.from_pretrained(model_name)

# # Sample texts (replace with your own texts)
# text1 = "Climate change is a pressing global issue."
# text2 = "Global warming is a major concern for our planet."

# # Tokenize the texts
# inputs1 = tokenizer(text1, return_tensors="pt", padding=True, truncation=True)
# inputs2 = tokenizer(text2, return_tensors="pt", padding=True, truncation=True)

# # Generate embeddings
# with torch.no_grad():
#     embeddings1 = model(**inputs1).last_hidden_state.mean(dim=1).numpy()
#     embeddings2 = model(**inputs2).last_hidden_state.mean(dim=1).numpy()

# # Calculate cosine similarity
# similarity = cosine_similarity(embeddings1, embeddings2)[0][0]

# print(f"Cosine Similarity: {similarity:.4f}")

# from sentence_transformers import SentenceTransformer, util

# combined_data_dedup = combined_data.drop_duplicates(subset=['processed_combined'],ignore_index=True)

# combined_data_dedup = combined_data_dedup.head(30)

# combined_data_dedup = test_df

# model = SentenceTransformer('all-MiniLM-L6-v2')
# def get_answer(row,all_activities):

#   input_sentence = row
#   embedding1 = model.encode(input_sentence, convert_to_tensor=True)
#   vijeta = -1
#   activity = ''
#   scores = []
#   for sentence in all_activities:
#     possible_match = sentence
#     embedding2 = model.encode(possible_match, convert_to_tensor=True)
#     cosine_similarity = util.cos_sim(embedding1, embedding2)
#     scores.append(cosine_similarity)
#     if vijeta < cosine_similarity:
#       vijeta = cosine_similarity
#       activity = possible_match
#   #print(scores)
#   print(vijeta)
#   print(activity.split(' _ ')[-1])
#   return activity.split(' _ ')[-1]


# combined_data_dedup['prediction'] = combined_data_dedup.apply(lambda x : get_answer(x['processed_project_desc'],all_activities),axis=1)

# print(combined_data_dedup[combined_data_dedup['activity']==combined_data_dedup['prediction']].shape)



EU_guide.head(1)

string = ['Construction or operation of electricity generation facilities that produce electricity using solar photovoltaic (PV) technology.Where an economic activity is an integral element of the ‘Installation,maintenance and repair of renewable energy technologies’ as referred to in Section 7.6 of this Annex,the technical screening criteria specified in Section 7.6 apply.The economic activities in this category could be associated with several NACE codes,in particular D35.11 and F42.22 in accordance with the statistical classification of economic activities established by Regulation (EC) No 1893/2006.',
          'Development of building projects for residential and non-residential buildings by bringing together financial,technical and physical means to realise the building projects for later sale as well as the construction of complete residential or non-residential buildings,on own account for sale or on a fee or contract basis.The economic activities in this category could be associated with several NACE codes,in particular F41.1 and F41.2,including also activities under F43,in accordance with the statistical classification of economic activities established by Regulation (EC) No 1893/2006.']
EU_guide[EU_guide['Description'].isin(string)].head(2)

EU_guide['Description'] = EU_guide['Description'].astype('str')

string = ['•\tWhere the very-low-carbon electricity is renewable energy, it shall meet the same criteria as in Energy sector activity: Generation of renewable energy with low lifecycle GHG emissions to supply electricity, heating, mechanical energy or cooling and activity: Joint use of renewable energy and fossil fuel to supply electricity, heat, mechanical energy or cooling.\n•\tA transmission or distribution project dedicated to the evacuation of only very-low-carbon electricity shall be fully eligible, with the exception of dedicated evacuation of new nuclear power generation.\n•\tA transmission or distribution project dedicated to the evacuation of electricity from plants that do not generate very-low-carbon electricity shall not be eligible.\n•\tIn all other transmission or distribution projects, the entity applying the Common Principles shall demonstrate that the electricity system is increasing the share of non-nuclear very-low-carbon electricity use, or if the system is already dominated by very-low-carbon electricity and there is little scope for expanding the share of non-nuclear very-low-carbon electricity, the activity shall not decrease the current share.\n•\tIf the activity involves an interconnection between electricity systems, the entity applying the Common Principles shall demonstrate that the investment will not significantly increase GHG emissions over the short or medium term.\n•\tThe increasing share of non-nuclear very-low-carbon electricity shall be reflected in the most recent power system development plan covering a planning horizon of up to 10 years and the financing shall be apportioned using the projected share of very-low-carbon electricity in the electricity being transported in the entire electricity system in which the activity will be undertaken at the end of the planning horizon. If the system planning horizon does not extend to 10 years but there is an officially recognised decarbonisation plan for the electricity system that extends up to 10 years, and the decarbonisation plan is consistent with the power system development plan, then the financing can be apportioned using the projected share of very-low- carbon electricity in the electricity system at the end of 10 years in the decarbonisation plan.','•\tThe entity applying the Common Principles shall demonstrate a substantially lower carbon intensity or energy intensity of the greenfield manufacturing facility or greenfield supplementary equipment or production lines at an existing manufacturing facility against a selected benchmark.\n•\tThe financing provided for a greenfield facility shall be apportioned according to the share of the total finance devoted to enabling high efficiency in a manner consistent with the principles of conservativeness and granularity.\n•\tComponents of activities that use fossil fuels shall not be eligible.']
EU_guide[EU_guide['Criteria'].isin(string)].head(2)

EU_tax_guide = EU_guide.copy()
ground_truth_cols = []
predicted_cols = []
for i,col in enumerate(EU_guide.columns):
#  ground_truth_cols.append(col+'_GroundTruth')
  predicted_cols.append(col+'_Predicted')

#combined_data_ground.columns = ground_truth_cols
EU_tax_guide.columns = predicted_cols

EU_tax_guide.shape

EU_tax_guide.columns

EU_tax_guide = EU_tax_guide[['Sector_Predicted','Activity_Predicted','Description_Predicted',
                            'Substantial contribution criteria_Predicted','DNSH on Climate adaptation_Predicted',
                            'DNSH on Water_Predicted','DNSH on Circular economy_Predicted',
                            'DNSH on Pollution prevention_Predicted','DNSH on Biodiversity_Predicted',
                            'Footnotes_Predicted']]

type(EU_tax_guide['Description_Predicted'][0])

#combined_data_ground = combined_data_ground.drop_duplicates()
EU_tax_guide =EU_tax_guide.drop_duplicates()

EU_tax_guide.columns

EU_tax_guide.shape

All_predictions_df = processed_test.copy()

#All_predictions_df['criteria_class_prediction'] = All_predictions_df.apply(lambda x : x['Prediction'].split('__')[-1],axis=1)
All_predictions_df['description_prediction'] = All_predictions_df['Prediction'].astype('str')
EU_tax_guide['Description_Predicted'] = EU_tax_guide['Description_Predicted'].astype('str')

#All_predictions_df['criteria_class_groundtruth'] = All_predictions_df.apply(lambda x : x['Label'].split('__')[-1],axis=1)
#All_predictions_df['criteria_class_groundtruth'] = All_predictions_df['criteria_class'].astype('int')

All_predictions_df.head(2)

EU_tax_guide.head(2)

EU_tax_guide.shape

#######------Reverse Mapping using training data-----#######

# All_predictions_df = processed_test.copy()

# combined_data_refined = combined_data[['sector', 'activity', 'description',
#        'metrics & thresholds', 'adaptation', 'water', 'circular economy',
#        'pollution', 'ecosystems','description_class']]
# temp_cols = ['sector', 'activity', 'description',
#        'metrics & thresholds', 'adaptation', 'water', 'circular economy',
#        'pollution', 'ecosystems']

# for column in temp_cols:
#   combined_data_refined[column] = combined_data_refined.apply(lambda x : '' if pd.isna(x['description']) else x[column],axis=1)


# # combined_data_refined['sector'] = combined_data_refined.apply(lambda x : '' if pd.isna(x['description']) else x['sector'],axis=1)
# # combined_data_refined['activity'] = combined_data_refined.apply(lambda x : '' if pd.isna(x['description']) else x['activity'],axis=1)

# # combined_data_ground = combined_data_refined.copy()
# combined_data_predicted = combined_data_refined.copy()

# ground_truth_cols = []
# predicted_cols = []
# for i,col in enumerate(combined_data_refined.columns):
#   # ground_truth_cols.append(col+'_GroundTruth')
#   predicted_cols.append(col+'_Predicted')

# # combined_data_ground.columns = ground_truth_cols
# combined_data_predicted.columns = predicted_cols

# combined_data_ground = combined_data_ground.drop_duplicates()
# combined_data_predicted = combined_data_predicted.drop_duplicates()

# combined_data_predicted['Description_Predicted'].unique()

# All_predictions_df['description_prediction'] = All_predictions_df['Prediction'].astype('str')
# combined_data_predicted['Description_Predicted'] = combined_data_predicted['description_Predicted'].astype('str')

# All_predictions_df.shape

# #####----for reverse mapping using combined data only------######

# #All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='criteria_class_groundtruth',right_on='criteria_class_GroundTruth',how='left')
# All_predictions_df_joined = All_predictions_df.merge(combined_data_predicted,left_on='description_prediction',right_on='description_Predicted',how='left')

All_predictions_df_joined.shape



import re

All_predictions_df['description_prediction_katahua'] = All_predictions_df.apply(lambda x : ' '.join(re.findall(r'\b\w+\b', x['description_prediction'])),axis=1)
EU_tax_guide['Description_Predicted_katahua'] = EU_tax_guide.apply(lambda x : ' '.join(re.findall(r'\b\w+\b', x['Description_Predicted'])),axis=1)

All_predictions_df[['description_prediction_katahua','description_prediction']].head(2)

#####----for dummy data------######

#All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='criteria_class_groundtruth',right_on='criteria_class_GroundTruth',how='left')
All_predictions_df_joined = All_predictions_df.merge(EU_tax_guide,left_on='description_prediction',right_on='Description_Predicted',how='left')

All_predictions_df_joined = All_predictions_df.merge(EU_tax_guide,left_on='description_prediction_katahua',right_on='Description_Predicted_katahua',how='left')

#####-----Don't RUN-------####

#All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='criteria_class_groundtruth',right_on='criteria_class_GroundTruth',how='left')
All_predictions_df_joined = All_predictions_df.merge(combined_data_predicted,left_on='criteria_prediction',right_on='Criteria_Predicted',how='left')

All_predictions_df_joined.shape

y_pred

array([11,  0, 17,  0,  0,
       17, 17, 13,  0, 12,
       13, 11, 31, 31,  0,
       7, 31,  37, 12, 13,
       12,  0, 13, 17, 31])
array([0.8391365 , 0.561139  , 0.8617287 , 0.74692476, 0.6166683 ,
       0.40945786, 0.5474665 , 0.7206632 , 0.93353695, 0.9807747 ,
       0.9976369 , 0.84422934, 0.21428262, 0.3636941 , 0.18330717,
       0.15765987, 0.98028827, 0.8843099 , 0.27168646, 0.7311989 ,
       0.6788339 , 0.4714743 , 0.7694457 , 0.29225093, 0.19194694],
      dtype=float32)

All_predictions_df_joined.head(30)

display(print(All_predictions_df_joined[['Activity','Activity_Predicted']].value_counts()))

All_predictions_df_joined.fillna('',inplace=True)









All_predictions_df_joined.columns



####------TEST DATA PREPROCESS-----######

final_test = All_predictions_df_joined.copy()
final_test['sec_cat_act_cri'] = final_test.apply(lambda row : str(row['Sector_Predicted']) + ' '+ str(row['Activity_Predicted']) + str(row['Description_Predicted']) ,axis=1)
final_test['processed_sec_cat_act_cri'] = final_test.loc[:, 'sec_cat_act_cri'].apply(lambda x: ' '.join(simple_preprocess(x)))
#final_test['processed_sec_cat_act_cri'] = final_test.loc[:, 'sec_cat_act_cri'].apply(clean_and_process_text)

####------TRAINING DATA PREPROCESS-----######

final_train = combined_data_final.copy()
final_train.fillna("",inplace=True)
final_train['descrpt'] = final_train.apply(lambda row : str(row['sector']) + ' '+ str(row['activity']) + ' ' + str(row['description']),axis=1)
final_train['processed_descrpt'] = final_train.loc[:, 'descrpt'].apply(lambda x: ' '.join(simple_preprocess(x)))
# final_train['processed_descrpt'] = final_train.loc[:, 'descrpt'].apply(clean_and_process_text)



import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)

X_train = final_train['processed_descrpt']
y_train = final_train['final assessment cbi comments']

X_test = final_test['processed_sec_cat_act_cri']
# y_test = final_test['Final Assessment CBI']

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
xgb_classifier = xgb.XGBClassifier()
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
print('6')
y_pred = le.inverse_transform(y_pred)
len(y_pred)
print('7')

final_test['Final_Assessment_Prediction'] = y_pred
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

final_test.head(2)

1final_test[final_test['Final Assessment CBI']==final_test['Final_Assessment_Prediction']].shape[0]/final_test.shape[0]

final_test.to_excel('./Test_Prediction_Output_Dummy_EU.xlsx')













#####--------ICMA Validation----------######

test = pd.ExcelFile('/content/Dummy_Data_Testing.xlsx')
test_df = pd.read_excel(test)

ICMA_data = pd.ExcelFile('./ICMA_Dataset_Final.xlsx')
ls_icma = ICMA_data.sheet_names[1:]

lis_icma=[]

for i in ls_icma:
  print(i)
  df_t = pd.read_excel(ICMA_data,i)
  lis_icma.append(df_t)

for idx, df in enumerate(lis_icma):
    lis_icma[idx] = df.drop_duplicates()

sheet_data_list = []

for i in range(len(lis_icma)):
  print(i)
  data = lis_icma[i]
  data.columns = [(x.lower().strip()) for x in data.columns]
  sheet_data_list.append(data)

combined_data = pd.concat(sheet_data_list,ignore_index=True)

combined_data.head(1)

description_dict = {}
all_descriptions = set(combined_data['description'].tolist())
max_class = -1
for i,val in enumerate(all_descriptions):
  description_dict[val] = i
  max_class = max(max_class,i)

def assign_description(row):
  if row['description'] in description_dict.keys():
    return description_dict[row['description']]
  else:
    return max_class

input_cols = ['industry','project name','asset type', 'short description',
       'detailed description','description_class','description','final assessment cbi']

output_cols = ['principle','category','description']

combined_data['description_class'] = combined_data.apply(lambda row : assign_description(row), axis=1)

combined_data.head(2)

combined_data.shape

# combined_data.fillna('',inplace=True)

combined_data_final = combined_data

combined_data_final.fillna('',inplace=True)

combined_data_final.shape

combined_data['final assessment cbi'].unique()

correct_string = 'Potentially aligned if compliant with screening indicator'
variations_to_replace = ['Aligned if compliant with the principles and its description',
                         'Compatible if compliant with the principles and its description',
                         'Compatible if compliant with screening indicator or if more information is found \n']

def replace_variations(text):
    if text in variations_to_replace:
        return correct_string
    else:
        return text

combined_data_final['final assessment cbi'] = combined_data_final['final assessment cbi'].apply(replace_variations)

combined_data_final['final assessment cbi'].replace('Automatically aligned\n','Automatically aligned',inplace= True)
combined_data_final['final assessment cbi'].replace('Not Compatible (not aligned)','Not aligned',inplace= True)
combined_data_final['final assessment cbi'].replace('Not Compatible ','Not aligned',inplace= True)
combined_data_final['final assessment cbi'].replace('Not Compatible','Not aligned',inplace= True)
combined_data_final['final assessment cbi'].replace('Insufficient guidance available* (principles not developed)',
                      'Insufficient guidance available (criteria not developed)',inplace= True)
combined_data_final['final assessment cbi'].replace('Insufficient detail available to determaine alignment',
                      'Insufficient detail available to determine alignment',inplace= True)
combined_data_final['final assessment cbi'].replace('Insufficient detail available to determine compatibility',
                      'Insufficient detail available to determine alignment',inplace= True)
combined_data_final['final assessment cbi'].replace('Insufficient guidance available* (criteria not developed)',
                      'Insufficient detail available to determine alignment',inplace= True)

combined_data_final['final assessment cbi'].unique()

combined_data_final.shape

input_data = combined_data_final[input_cols]

output = combined_data_final[output_cols]



####------TEST DATA PREPROCESS-----######
####------For 348 Projects---------######


test_df.columns = ['Sector_input', 'Category_input', 'Detailed description', 'Sector', 'Category',
       'Activity', 'Criteria', 'Final Assessment CBI']
processed_test = test_df
processed_test['project_desc'] = processed_test.apply(lambda row : str(row['Sector_input']) + ' '+ str(row['Detailed description']) ,axis=1)
processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(lambda x: ' '.join(simple_preprocess(x)))
#processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(clean_and_process_text)

####------TEST DATA PREPROCESS-----######
####------For Dummy Data---------######


test_df.columns = ['Project Number', 'Industry', 'Project Name', 'Asset Type','Short description',
       'Detailed description','Type']
processed_test = test_df
processed_test['project_desc'] = processed_test.apply(lambda row : str(row['Industry']) +' '+ str(row['Asset Type'])+ ' ' + str(row['Short description']) ,axis=1)
processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(lambda x: ' '.join(simple_preprocess(x)))
#processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(clean_and_process_text)

input_data.head(2)

####------TRAINING DATA PREPROCESS-----######

project_name_asset_data = input_data
project_name_asset_data.fillna("",inplace=True)

# with short and detailed description
project_name_asset_data['project_asset'] = project_name_asset_data.apply(lambda row : str(row['industry'])+' '+str(row['project name']) + ' '+ str(row['asset type']) + ' ' + str(row['short description'])+ ' ' + str(row['detailed description']),axis=1)
project_name_asset_data['processed_project_asset'] = project_name_asset_data.loc[:, 'project_asset'].apply(lambda x: ' '.join(simple_preprocess(x)))
# project_name_asset_data['processed_project_asset'] = project_name_asset_data.loc[:, 'project_asset'].apply(clean_and_process_text)

project_name_asset_data.shape

project_name_asset_data.columns

project_name_asset_data.head(2)



project_name_asset_data = project_name_asset_data.drop_duplicates(subset=['processed_project_asset'],ignore_index=True)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)
project_name_asset_data = project_name_asset_data.drop_duplicates(subset=['processed_project_asset'],ignore_index=True)
project_name_asset_data_ = project_name_asset_data[project_name_asset_data['description']!='']
project_name_asset_data__ = project_name_asset_data[project_name_asset_data['description']==''].head(900)
project_name_asset_data = pd.concat([project_name_asset_data_,project_name_asset_data__])


# Determine the target number of samples per class (e.g., the number of samples in the minority class)
# target_samples_per_class = project_name_asset_data['description'].value_counts().max()
# target_samples_per_class = 300

# Initialize an empty DataFrame for the balanced dataset
# balanced_df = pd.DataFrame(columns=project_name_asset_data.columns)

# # Oversample minority classes and undersample the majority class
# for class_label in project_name_asset_data['description'].unique():
#     class_data = project_name_asset_data[project_name_asset_data['description'] == class_label]
#     if len(class_data) < target_samples_per_class:
#         # Oversample by duplicating samples
#         num_duplicates = target_samples_per_class // len(class_data)
#         oversampled_class_data = pd.concat([class_data] * num_duplicates, ignore_index=True)
#         balanced_df = pd.concat([balanced_df, oversampled_class_data])
#     else:
#         # Undersample by using all samples
#         balanced_df = pd.concat([balanced_df, class_data.sample(target_samples_per_class)])

# print(balanced_df)

# project_name_asset_data = balanced_df

X_train = project_name_asset_data['processed_project_asset']
y_train = project_name_asset_data['description']

X_test = processed_test['processed_project_desc']
#y_test_a = processed_test['Criteria'].to_numpy()

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=10000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
from sklearn.utils import class_weight
classes_weights = list(class_weight.compute_class_weight(class_weight = 'balanced',
                                             classes = np.unique(project_name_asset_data['description']),
                                             y = project_name_asset_data['description']))

weights = np.ones(y_train.shape[0], dtype = 'float')
for i, val in enumerate(y_train):
    weights[i] = classes_weights[val-1]

xgb_classifier = xgb.XGBClassifier(sample_weight=weights)
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
y_pred_proba = xgb_classifier.predict_proba(X_test_tfidf)
max_class_probabilities = np.max(y_pred_proba, axis=1)

print('6')
y_pred_transform = le.inverse_transform(y_pred)
len(y_pred_transform)
print('7')

processed_test['Prediction'] = y_pred_transform
processed_test['Probability'] = max_class_probabilities
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

processed_test['Prediction'] = processed_test['Prediction'].replace('\n','', regex=True)

processed_test['Prediction'] = processed_test['Prediction'].str.replace(r'\s*([,.])\s*', r'\1', regex=True)

Tax_Guide = pd.ExcelFile('/content/Taxonomies_and_principles.xlsx')

EU_guide = pd.read_excel(Tax_Guide, 'ICMA Taxonomy')

processed_test['Prediction'] = processed_test['Prediction'].str.strip()
EU_guide['Description'] = EU_guide['Description'].str.strip()

EU_guide['Description'] = EU_guide['Description'].str.replace(r'\s*([,.])\s*', r'\1', regex=True)

EU_guide.head(1)

EU_guide['combined'] = EU_guide.apply(lambda x : str(x['Principle']) + " _ "+str(x['Categories']) \
                          + " _ "+str(x['Description']),axis=1)

EU_guide['combined'].head()

combined_data['processed_combined'] = combined_data.apply(lambda x : str(x['industry']) + " " +str(x['project name']) + " " + str(x['asset type']) + " "+str(x['short description']) \
                          + " "+str(x['detailed description']),axis=1)

all_activities = EU_guide['combined'].tolist()

!pip install -U sentence_transformers

from sentence_transformers import SentenceTransformer, util

combined_data_dedup = combined_data.drop_duplicates(subset=['processed_combined'],ignore_index=True)

combined_data_dedup = combined_data_dedup.head(30)

combined_data_dedup = test_df

model = SentenceTransformer('all-MiniLM-L6-v2')
def get_answer(row,all_activities):

  input_sentence = row
  embedding1 = model.encode(input_sentence, convert_to_tensor=True)
  vijeta = -1
  activity = ''
  scores = []
  for sentence in all_activities:
    possible_match = sentence
    embedding2 = model.encode(possible_match, convert_to_tensor=True)
    cosine_similarity = util.cos_sim(embedding1, embedding2)
    scores.append(cosine_similarity)
    if vijeta < cosine_similarity:
      vijeta = cosine_similarity
      activity = possible_match
  #print(scores)
  print(vijeta)
  print(activity.split(' _ ')[-2])
  return pd.Series([activity.split(' _ ')[-2],vijeta.cpu().numpy()[0][0]])

combined_data_dedup[['mini_LM_prediction','cosine_sim']] = combined_data_dedup.apply(lambda x : get_answer(x['processed_project_desc'],all_activities),axis=1)

combined_data_dedup.head()

combined_data_dedup['ensemble_prediction'] = combined_data_dedup.apply(lambda x : x['Prediction'] if x['Probability']>0.9 \
                                                                      else (x['mini_LM_prediction'] if x['cosine_sim']>0.3 else ''),axis=1)

combined_data_dedup.shape

combined_data_dedup

combined_data_dedup[['Project Number','ensemble_prediction']]















import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)
project_name_asset_data_ = project_name_asset_data[project_name_asset_data['description']!='']
project_name_asset_data__ = project_name_asset_data[project_name_asset_data['description']==''].head(600)
project_name_asset_data = pd.concat([project_name_asset_data_,project_name_asset_data__])

X_train = project_name_asset_data['processed_project_asset']
y_train = project_name_asset_data['description']

X_test = processed_test['processed_project_desc']
#y_test_a = processed_test['Criteria'].to_numpy()

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
xgb_classifier = xgb.XGBClassifier()
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
print('6')
y_pred_transform = le.inverse_transform(y_pred)
len(y_pred_transform)
print('7')

processed_test['Prediction'] = y_pred_transform
# Evaluate the model
# print("Classification Report:")
#print(classification_report(y_test, y_pred))

print(processed_test.head(30)['Prediction'].unique())

processed_test.head(30)

# processed_test['Prediction'] = processed_test['Prediction'].replace('\n','', regex=True)

Tax_Guide = pd.ExcelFile('/content/Taxonomies_and_principles.xlsx')

ICMA_guide = pd.read_excel(Tax_Guide, 'ICMA Taxonomy')

ICMA_guide.head(1)

string = ['Such as the design and introduction of reusable, recyclable and refurbished materials, components and products; circular tools and services; and/or certified eco-efficient products']
ICMA_guide[ICMA_guide['Description'].isin(string)].head(2)

ICMA_guide['Description'] = ICMA_guide['Description'].astype('str')

ICMA_tax_guide = ICMA_guide.copy()
ground_truth_cols = []
predicted_cols = []
for i,col in enumerate(ICMA_guide.columns):
#  ground_truth_cols.append(col+'_GroundTruth')
  predicted_cols.append(col+'_Predicted')

#combined_data_ground.columns = ground_truth_cols
ICMA_tax_guide.columns = predicted_cols

ICMA_tax_guide.shape

ICMA_tax_guide.columns

# ICMA_tax_guide = ICMA_tax_guide[['Sector_Predicted','Activity_Predicted','Description_Predicted',
#                             'Substantial contribution criteria_Predicted','DNSH on Climate adaptation_Predicted',
#                             'DNSH on Water_Predicted','DNSH on Circular economy_Predicted',
#                             'DNSH on Pollution prevention_Predicted','DNSH on Biodiversity_Predicted',
#                             'Footnotes_Predicted']]

type(ICMA_tax_guide['Description_Predicted'][0])

#combined_data_ground = combined_data_ground.drop_duplicates()
ICMA_tax_guide =ICMA_tax_guide.drop_duplicates()

ICMA_tax_guide.columns

ICMA_tax_guide.shape

All_predictions_df = processed_test.copy()

#All_predictions_df['criteria_class_prediction'] = All_predictions_df.apply(lambda x : x['Prediction'].split('__')[-1],axis=1)
All_predictions_df['description_prediction'] = All_predictions_df['Prediction'].astype('str')
ICMA_tax_guide['Description_Predicted'] = ICMA_tax_guide['Description_Predicted'].astype('str')

#All_predictions_df['criteria_class_groundtruth'] = All_predictions_df.apply(lambda x : x['Label'].split('__')[-1],axis=1)
#All_predictions_df['criteria_class_groundtruth'] = All_predictions_df['criteria_class'].astype('int')

All_predictions_df.head(30)

ICMA_tax_guide.head(2)

ICMA_tax_guide.shape

All_predictions_df.shape

#####----for dummy data------######

#All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='criteria_class_groundtruth',right_on='criteria_class_GroundTruth',how='left')
All_predictions_df_joined = All_predictions_df.merge(ICMA_tax_guide,left_on='description_prediction',right_on='Description_Predicted',how='left')

#####-----Don't RUN-------####

#All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='criteria_class_groundtruth',right_on='criteria_class_GroundTruth',how='left')
All_predictions_df_joined = All_predictions_df.merge(combined_data_predicted,left_on='criteria_prediction',right_on='Criteria_Predicted',how='left')

All_predictions_df_joined.shape

All_predictions_df_joined.head(3)

display(print(All_predictions_df_joined[['Activity','Activity_Predicted']].value_counts()))

All_predictions_df_joined.fillna('',inplace=True)









All_predictions_df_joined.columns



####------TEST DATA PREPROCESS-----######

final_test = All_predictions_df_joined.copy()
final_test['sec_cat_act_cri'] = final_test.apply(lambda row : str(row['Principle_Predicted']) + ' '+ str(row['Categories_Predicted']) + str(row['Description_Predicted']) ,axis=1)
final_test['processed_sec_cat_act_cri'] = final_test.loc[:, 'sec_cat_act_cri'].apply(lambda x: ' '.join(simple_preprocess(x)))
#final_test['processed_sec_cat_act_cri'] = final_test.loc[:, 'sec_cat_act_cri'].apply(clean_and_process_text)

####------TRAINING DATA PREPROCESS-----######

final_train = combined_data_final.copy()
final_train.fillna("",inplace=True)
final_train['descrpt'] = final_train.apply(lambda row : str(row['principle']) + ' '+ str(row['category']) + ' ' + str(row['description']),axis=1)
final_train['processed_descrpt'] = final_train.loc[:, 'descrpt'].apply(lambda x: ' '.join(simple_preprocess(x)))
# final_train['processed_descrpt'] = final_train.loc[:, 'descrpt'].apply(clean_and_process_text)



import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)

X_train = final_train['processed_descrpt']
y_train = final_train['final assessment cbi']

X_test = final_test['processed_sec_cat_act_cri']
# y_test = final_test['Final Assessment CBI']

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
xgb_classifier = xgb.XGBClassifier()
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
print('6')
y_pred = le.inverse_transform(y_pred)
len(y_pred)
print('7')

final_test['Final_Assessment_Prediction'] = y_pred
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

final_test.head(5)

1final_test[final_test['Final Assessment CBI']==final_test['Final_Assessment_Prediction']].shape[0]/final_test.shape[0]

final_test.to_excel('./Test_Prediction_Output_Dummy_ICMA.xlsx')













#####--------CBI Validation----------######

test = pd.ExcelFile('/content/Dummy_Data_Testing.xlsx')
test_df = pd.read_excel(test)

CBI_data = pd.ExcelFile('./CBI_Combined_Data.xlsx')

CBI_data.sheet_names

combined_data = pd.read_excel(CBI_data,'Combined Data CBI')

ls_cbi = CBI_data.sheet_names[1:]

lis_cbi=[]

for i in ls_cbi:
  print(i)
  df_t = pd.read_excel(CBI_data,i)
  lis_cbi.append(df_t)

for idx, df in enumerate(lis_cbi):

    lis_cbi[idx] = df.drop_duplicates().reset_index(drop=True)

sheet_data_list = []

for i in range(len(lis_cbi)):
  cols = [(x) for x in lis_cbi[i].columns if x not in ['Social Impact Indicators (Social Options)','SDG\nAlignment (Green Options)','Green Impact Indicators (Options)']]
  data = lis_cbi[i][cols]
  data.columns = [(x.lower().strip()) for x in data.columns]
  sheet_data_list.append(data)
  #sheet_data_list[i].reset_index()

combined_data = pd.concat(sheet_data_list, ignore_index = True)

#combined_data.to_excel('CBI_Combined_Data.xlsx',index=False,header=True)

combined_data.fillna(' ',inplace=True)

combined_data.head()

combined_data['description'] = combined_data['sector'] + '_' + combined_data['subsector'] + '_' + combined_data['asset type'] + '_' + combined_data['asset specifics']

combined_data.head(1)

description_dict = {}
all_descriptions = set(combined_data['description'].tolist())
max_class = -1
for i,val in enumerate(all_descriptions):
  description_dict[val] = i
  max_class = max(max_class,i)

def assign_description(row):
  if row['description'] in description_dict.keys():
    return description_dict[row['description']]
  else:
    return max_class

input_cols = ['industry','project name','asset type_input', 'short description', 'type','industry',
       'detailed description','description_class','description','final assessment cbi comments']

output_cols = ['sector', 'subsector', 'asset type',
       'asset specifics', 'final assessment cbi comments']

combined_data['description_class'] = combined_data.apply(lambda row : assign_description(row), axis=1)

combined_data.head(2)

combined_data.shape

# combined_data.fillna('',inplace=True)

combined_data_final = combined_data

combined_data_final.fillna('',inplace=True)

combined_data_final.shape



correct_string = 'Potentially aligned if compliant with screening indicator'
variations_to_replace = ['Compatible if compliant with screening indicator or if more information is found',
                         'Compatible if complient with screening indicator or if more information is found\nEligible for CBI Certification']

def replace_variations(text):
    if text in variations_to_replace:
        return correct_string
    else:
        return text

combined_data_final['final assessment cbi comments'] = combined_data_final['final assessment cbi comments'].apply(replace_variations)

combined_data_final['final assessment cbi comments'].replace('Automatically compatible\n','Automatically aligned',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Automatically compatible\nEligible for CBI Certification','Automatically aligned',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Not Compatible (not aligned)','Not aligned',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Not Compatible ','Not aligned',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Not compatible ','Not aligned',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Not Compatible','Not aligned',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Not Aligned','Not aligned',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Not yet ready for  CBI certification\nCriteria likely to be finalised in the next months\nAsset/project can be included in the Green Bond but not eligible for certification',
                      'Insufficient guidance available (criteria not developed)',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Insufficient detail available to determine compatibility',
                      'Insufficient detail available to determine alignment',inplace= True)
combined_data_final['final assessment cbi comments'].replace('Insufficient guidance available* (criteria not developed)',
                      'Insufficient guidance available (criteria not developed)',inplace= True)

combined_data_final['final assessment cbi comments'].unique()

combined_data_final.shape

input_data = combined_data_final[input_cols]

output = combined_data_final[output_cols]



####------TEST DATA PREPROCESS-----######
####------For 348 Projects---------######


test_df.columns = ['Sector_input', 'Category_input', 'Detailed description', 'Sector', 'Category',
       'Activity', 'Criteria', 'Final Assessment CBI']
processed_test = test_df
processed_test['project_desc'] = processed_test.apply(lambda row : str(row['Sector_input']) + ' '+ str(row['Detailed description']) ,axis=1)
processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(lambda x: ' '.join(simple_preprocess(x)))
#processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(clean_and_process_text)

####------TEST DATA PREPROCESS-----######
####------For Dummy Data---------######


test_df.columns = ['Project Number', 'Industry', 'Project Name', 'Asset Type','Short description',
       'Detailed description','Type']
processed_test = test_df
processed_test['project_desc'] = processed_test.apply(lambda row : str(row['Industry']) +' '+ str(row['Asset Type'])+ ' ' + str(row['Short description']) ,axis=1)
processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(lambda x: ' '.join(simple_preprocess(x)))
#processed_test['processed_project_desc'] = processed_test.loc[:, 'project_desc'].apply(clean_and_process_text)

input_data.head(2)

project_name_asset_data.head()

####------TRAINING DATA PREPROCESS-----######

project_name_asset_data = input_data
project_name_asset_data.fillna("",inplace=True)

# with short and detailed description
project_name_asset_data['project_asset'] = project_name_asset_data.apply(lambda row : str(row['industry'])+' '+str(row['project name']) + ' '+ str(row['asset type_input']) + ' ' + str(row['short description'])+ ' ' + str(row['detailed description']),axis=1)
project_name_asset_data['processed_project_asset'] = project_name_asset_data.loc[:, 'project_asset'].apply(lambda x: ' '.join(simple_preprocess(x)))
# project_name_asset_data['processed_project_asset'] = project_name_asset_data.loc[:, 'project_asset'].apply(clean_and_process_text)

project_name_asset_data.shape

project_name_asset_data.columns

project_name_asset_data.head(2)

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)
project_name_asset_data = project_name_asset_data.drop_duplicates(subset=['processed_project_asset'],ignore_index=True)
project_name_asset_data_ = project_name_asset_data[project_name_asset_data['description']!='']
project_name_asset_data__ = project_name_asset_data[project_name_asset_data['description']==''].head(900)
project_name_asset_data = pd.concat([project_name_asset_data_,project_name_asset_data__])


# Determine the target number of samples per class (e.g., the number of samples in the minority class)
# target_samples_per_class = project_name_asset_data['description'].value_counts().max()
# target_samples_per_class = 300

# Initialize an empty DataFrame for the balanced dataset
# balanced_df = pd.DataFrame(columns=project_name_asset_data.columns)

# # Oversample minority classes and undersample the majority class
# for class_label in project_name_asset_data['description'].unique():
#     class_data = project_name_asset_data[project_name_asset_data['description'] == class_label]
#     if len(class_data) < target_samples_per_class:
#         # Oversample by duplicating samples
#         num_duplicates = target_samples_per_class // len(class_data)
#         oversampled_class_data = pd.concat([class_data] * num_duplicates, ignore_index=True)
#         balanced_df = pd.concat([balanced_df, oversampled_class_data])
#     else:
#         # Undersample by using all samples
#         balanced_df = pd.concat([balanced_df, class_data.sample(target_samples_per_class)])

# print(balanced_df)

# project_name_asset_data = balanced_df

X_train = project_name_asset_data['processed_project_asset']
y_train = project_name_asset_data['description']

X_test = processed_test['processed_project_desc']
#y_test_a = processed_test['Criteria'].to_numpy()

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=10000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
from sklearn.utils import class_weight
classes_weights = list(class_weight.compute_class_weight(class_weight = 'balanced',
                                             classes = np.unique(project_name_asset_data['description']),
                                             y = project_name_asset_data['description']))

weights = np.ones(y_train.shape[0], dtype = 'float')
for i, val in enumerate(y_train):
    weights[i] = classes_weights[val-1]

xgb_classifier = xgb.XGBClassifier(sample_weight=weights)
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
y_pred_proba = xgb_classifier.predict_proba(X_test_tfidf)
max_class_probabilities = np.max(y_pred_proba, axis=1)

print('6')
y_pred_transform = le.inverse_transform(y_pred)
len(y_pred_transform)
print('7')

processed_test['Prediction'] = y_pred_transform
processed_test['Probability'] = max_class_probabilities
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

processed_test['Prediction'] = processed_test['Prediction'].replace('\n','', regex=True)

processed_test['Prediction'] = processed_test['Prediction'].str.replace(r'\s*([,.])\s*', r'\1', regex=True)

Tax_Guide = pd.ExcelFile('/content/Taxonomies_and_principles.xlsx')

Tax_Guide.sheet_names

EU_guide = pd.read_excel(Tax_Guide, 'CBI sector criteria')

EU_guide.columns = EU_guide.iloc[0]
EU_guide = EU_guide.iloc[1:]

EU_guide.head()

EU_guide.shape

EU_guide['Description'] = EU_guide['Sector'] + " _ " + EU_guide['Subsector'] + " _ " + EU_guide['Asset type'] + " _ " + EU_guide['Asset specifics ']

processed_test['Prediction'] = processed_test['Prediction'].str.strip()
EU_guide['Description'] = EU_guide['Description'].str.strip()

EU_guide['Description'] = EU_guide['Description'].str.replace(r'\s*([,.])\s*', r'\1', regex=True)

EU_guide.head(1)

EU_guide['combined'] = EU_guide['Description']

EU_guide['combined'].head()

combined_data['processed_combined'] = combined_data.apply(lambda x : str(x['industry']) + " " +str(x['project name']) + " " + str(x['asset type']) + " "+str(x['short description']) \
                          + " "+str(x['detailed description']),axis=1)

all_activities = EU_guide['combined'].tolist()

!pip install -U sentence_transformers

from sentence_transformers import SentenceTransformer, util

combined_data_dedup = combined_data.drop_duplicates(subset=['processed_combined'],ignore_index=True)

combined_data_dedup = combined_data_dedup.head(30)

combined_data_dedup = test_df

model = SentenceTransformer('all-MiniLM-L6-v2')
def get_answer(row,all_activities):

  input_sentence = row
  embedding1 = model.encode(input_sentence, convert_to_tensor=True)
  vijeta = -1
  activity = ''
  scores = []
  for sentence in all_activities:
    possible_match = sentence
    embedding2 = model.encode(str(possible_match), convert_to_tensor=True)
    cosine_similarity = util.cos_sim(embedding1, embedding2)
    scores.append(cosine_similarity)
    if vijeta < cosine_similarity:
      vijeta = cosine_similarity
      activity = possible_match
  #print(scores)
  print(vijeta)
  print(activity)
  return pd.Series([activity,vijeta.cpu().numpy()[0][0]])

combined_data_dedup[['mini_LM_prediction','cosine_sim']] = combined_data_dedup.apply(lambda x : get_answer(x['processed_project_desc'],all_activities),axis=1)

combined_data_dedup.head()

combined_data_dedup['ensemble_prediction'] = combined_data_dedup.apply(lambda x : x['Prediction'] if x['Probability']>0.7 \
                                                                      else (x['mini_LM_prediction'] if x['cosine_sim']>0.3 else ''),axis=1)

combined_data_dedup.shape

combined_data_dedup

combined_data_dedup[['Project Number','ensemble_prediction']]

























import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)
project_name_asset_data_ = project_name_asset_data[project_name_asset_data['description']!='']
project_name_asset_data__ = project_name_asset_data[project_name_asset_data['description']==''].head(600)
project_name_asset_data = pd.concat([project_name_asset_data_,project_name_asset_data__])

X_train = project_name_asset_data['processed_project_asset']
y_train = project_name_asset_data['description']

X_test = processed_test['processed_project_desc']
#y_test_a = processed_test['Criteria'].to_numpy()

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
xgb_classifier = xgb.XGBClassifier()
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
print('6')
y_pred_transform = le.inverse_transform(y_pred)
len(y_pred_transform)
print('7')

processed_test['Prediction'] = y_pred_transform
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

print(processed_test.head(30)['Prediction'].unique())

processed_test.head(30)

########-------Reverse Mapping with combined training data-----######

All_predictions_df.head()

All_predictions_df = processed_test.copy()

combined_data_refined = combined_data[['sector', 'subsector', 'asset type',
       'asset specifics','description','description_class']]
temp_cols = ['sector', 'subsector', 'asset type',
       'asset specifics','description']

for column in temp_cols:
  combined_data_refined[column] = combined_data_refined.apply(lambda x : '' if pd.isna(x['description']) else x[column],axis=1)


# combined_data_refined['sector'] = combined_data_refined.apply(lambda x : '' if pd.isna(x['description']) else x['sector'],axis=1)
# combined_data_refined['activity'] = combined_data_refined.apply(lambda x : '' if pd.isna(x['description']) else x['activity'],axis=1)

combined_data_ground = combined_data_refined.copy()
combined_data_predicted = combined_data_refined.copy()

ground_truth_cols = []
predicted_cols = []
for i,col in enumerate(combined_data_refined.columns):
  ground_truth_cols.append(col+'_GroundTruth')
  predicted_cols.append(col+'_Predicted')

combined_data_ground.columns = ground_truth_cols
combined_data_predicted.columns = predicted_cols

combined_data_ground = combined_data_ground.drop_duplicates()
combined_data_predicted =combined_data_predicted.drop_duplicates()

combined_data_predicted.columns

combined_data_predicted.shape

combined_data_ground.shape

#All_predictions_df['description_class_prediction'] = All_predictions_df.apply(lambda x : x['Prediction'].split('__')[-1],axis=1)
All_predictions_df['description_prediction'] = All_predictions_df['Prediction'].astype('str')

#All_predictions_df['description_class_groundtruth'] = All_predictions_df.apply(lambda x : x['Label'].split('__')[-1],axis=1)
# All_predictions_df['description_groundtruth'] = All_predictions_df['description'].astype('str')

# All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='description_class_groundtruth',right_on='description_class_GroundTruth',how='left')
All_predictions_df_joined = All_predictions_df.merge(combined_data_predicted,left_on='description_prediction',right_on='description_Predicted',how='left')

All_predictions_df_joined.shape

All_predictions_df_joined.head()

#######-------End of reverse mapping with combined data---#########









######-------reverse mapping with taxonomy guidelines------########

# processed_test['Prediction'] = processed_test['Prediction'].replace('\n','', regex=True)

Tax_Guide = pd.ExcelFile('/content/Taxonomies_and_principles.xlsx')

CBI_guide = pd.read_excel(Tax_Guide, 'CBI sector criteria')

ICMA_guide.head(1)

string = ['Development of building projects for residential and non-residential buildings by bringing together financial, technical and physical means to realise the building projects for later sale as well as the construction of complete residential or non-residential buildings, on own account for sale or on a fee or contract basis.The economic activities in this category could be associated with several NACE codes, in particular F41.1 and F41.2, including also activities under F43, in accordance with the statistical classification of economic activities established by Regulation (EC) No 1893/2006.']
ICMA_guide[ICMA_guide['Description'].isin(string)].head(2)

ICMA_guide['Description'] = ICMA_guide['Description'].astype('str')

string = ['•\tWhere the very-low-carbon electricity is renewable energy, it shall meet the same criteria as in Energy sector activity: Generation of renewable energy with low lifecycle GHG emissions to supply electricity, heating, mechanical energy or cooling and activity: Joint use of renewable energy and fossil fuel to supply electricity, heat, mechanical energy or cooling.\n•\tA transmission or distribution project dedicated to the evacuation of only very-low-carbon electricity shall be fully eligible, with the exception of dedicated evacuation of new nuclear power generation.\n•\tA transmission or distribution project dedicated to the evacuation of electricity from plants that do not generate very-low-carbon electricity shall not be eligible.\n•\tIn all other transmission or distribution projects, the entity applying the Common Principles shall demonstrate that the electricity system is increasing the share of non-nuclear very-low-carbon electricity use, or if the system is already dominated by very-low-carbon electricity and there is little scope for expanding the share of non-nuclear very-low-carbon electricity, the activity shall not decrease the current share.\n•\tIf the activity involves an interconnection between electricity systems, the entity applying the Common Principles shall demonstrate that the investment will not significantly increase GHG emissions over the short or medium term.\n•\tThe increasing share of non-nuclear very-low-carbon electricity shall be reflected in the most recent power system development plan covering a planning horizon of up to 10 years and the financing shall be apportioned using the projected share of very-low-carbon electricity in the electricity being transported in the entire electricity system in which the activity will be undertaken at the end of the planning horizon. If the system planning horizon does not extend to 10 years but there is an officially recognised decarbonisation plan for the electricity system that extends up to 10 years, and the decarbonisation plan is consistent with the power system development plan, then the financing can be apportioned using the projected share of very-low- carbon electricity in the electricity system at the end of 10 years in the decarbonisation plan.','•\tThe entity applying the Common Principles shall demonstrate a substantially lower carbon intensity or energy intensity of the greenfield manufacturing facility or greenfield supplementary equipment or production lines at an existing manufacturing facility against a selected benchmark.\n•\tThe financing provided for a greenfield facility shall be apportioned according to the share of the total finance devoted to enabling high efficiency in a manner consistent with the principles of conservativeness and granularity.\n•\tComponents of activities that use fossil fuels shall not be eligible.']
ICMA_guide[ICMA_guide['Criteria'].isin(string)].head(2)

CBI_tax_guide = CBI_guide.copy()
ground_truth_cols = []
predicted_cols = []
for i,col in enumerate(EU_guide.columns):
#  ground_truth_cols.append(col+'_GroundTruth')
  predicted_cols.append(col+'_Predicted')

#combined_data_ground.columns = ground_truth_cols
EU_tax_guide.columns = predicted_cols

ICMA_tax_guide.shape

CBI_tax_guide.columns

CBI_tax_guide = CBI_tax_guide[['Sector_Predicted','Activity_Predicted','Description_Predicted',
                            'Substantial contribution criteria_Predicted','DNSH on Climate adaptation_Predicted',
                            'DNSH on Water_Predicted','DNSH on Circular economy_Predicted',
                            'DNSH on Pollution prevention_Predicted','DNSH on Biodiversity_Predicted',
                            'Footnotes_Predicted']]

type(CBI_tax_guide['Description_Predicted'][0])

#combined_data_ground = combined_data_ground.drop_duplicates()
CBI_tax_guide =CBI_tax_guide.drop_duplicates()

CBI_tax_guide.columns

CBI_tax_guide.shape

All_predictions_df = processed_test.copy()

#All_predictions_df['criteria_class_prediction'] = All_predictions_df.apply(lambda x : x['Prediction'].split('__')[-1],axis=1)
All_predictions_df['description_prediction'] = All_predictions_df['Prediction'].astype('str')
ICMA_tax_guide['Description_Predicted'] = ICMA_tax_guide['Description_Predicted'].astype('str')

#All_predictions_df['criteria_class_groundtruth'] = All_predictions_df.apply(lambda x : x['Label'].split('__')[-1],axis=1)
#All_predictions_df['criteria_class_groundtruth'] = All_predictions_df['criteria_class'].astype('int')

All_predictions_df.head(2)

ICMA_tax_guide.head(2)

ICMA_tax_guide.shape

All_predictions_df.shape

#####----for dummy data------######

#All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='criteria_class_groundtruth',right_on='criteria_class_GroundTruth',how='left')
All_predictions_df_joined = All_predictions_df.merge(CBI_tax_guide,left_on='description_prediction',right_on='Description_Predicted',how='left')

#####-----Don't RUN-------####

#All_predictions_df_joined = All_predictions_df.merge(combined_data_ground,left_on='criteria_class_groundtruth',right_on='criteria_class_GroundTruth',how='left')
All_predictions_df_joined = All_predictions_df.merge(combined_data_predicted,left_on='criteria_prediction',right_on='Criteria_Predicted',how='left')

All_predictions_df_joined.shape

All_predictions_df_joined.head(3)

display(print(All_predictions_df_joined[['Activity','Activity_Predicted']].value_counts()))

All_predictions_df_joined.fillna('',inplace=True)









All_predictions_df_joined.columns



####------TEST DATA PREPROCESS-----######

final_test = All_predictions_df_joined.copy()
final_test['sec_cat_act_cri'] = final_test.apply(lambda row : str(row['sector_Predicted']) + ' '+ str(row['subsector_Predicted']) + ' '+ str(row['asset type_Predicted']) + ' '+ str(row['asset specifics_Predicted']),axis=1)
final_test['processed_sec_cat_act_cri'] = final_test.loc[:, 'sec_cat_act_cri'].apply(lambda x: ' '.join(simple_preprocess(x)))
#final_test['processed_sec_cat_act_cri'] = final_test.loc[:, 'sec_cat_act_cri'].apply(clean_and_process_text)

####------TRAINING DATA PREPROCESS-----######

final_train = combined_data_final.copy()
final_train.fillna("",inplace=True)
final_train['descrpt'] = final_train.apply(lambda row : str(row['sector']) + ' '+ str(row['subsector']) + ' ' + str(row['asset type']) + ' ' + str(row['asset specifics']),axis=1)
final_train['processed_descrpt'] = final_train.loc[:, 'descrpt'].apply(lambda x: ' '.join(simple_preprocess(x)))
# final_train['processed_descrpt'] = final_train.loc[:, 'descrpt'].apply(clean_and_process_text)



import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder

# Load and preprocess your data if needed
# Assuming you've already loaded and preprocessed your data into a DataFrame 'df'.

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(project_name_asset_data['processed_project_asset'],project_name_asset_data['criteria'], test_size=0.2, random_state=42)

X_train = final_train['processed_descrpt']
y_train = final_train['final assessment cbi comments']

X_test = final_test['processed_sec_cat_act_cri']
# y_test = final_test['Final Assessment CBI']

# # Sample NumPy arrays with text values
# array1 = y_train_a
# array2 = y_test_a

# # Combine all text values from both arrays
# combined_array = np.concatenate([array1, array2])

# # Initialize a LabelEncoder
# label_encoder = LabelEncoder()

# # Fit the encoder on the combined array to assign labels consistently
# label_encoder.fit(combined_array)

# # Transform each array using the same encoder
# y_train = label_encoder.transform(array1)
# y_test = label_encoder.transform(array2)

# # The encoder will assign the same labels for the same text values
# print("Encoded Array 1:", y_train)
# print("Encoded Array 2:", y_test)

# # Inverse transform to get the original text values
# decoded_array1 = label_encoder.inverse_transform(y_train)
# decoded_array2 = label_encoder.inverse_transform(y_test)

# # Display the decoded arrays
# print("Decoded Array 1:", len(decoded_array1))
# print("Decoded Array 2:", len(decoded_array2))

le = LabelEncoder()
y_train = le.fit_transform(y_train)

# Create a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
print('1')
# Fit and transform the vectorizer on the training data
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
print('2')
# Transform the test data
X_test_tfidf = tfidf_vectorizer.transform(X_test)
print('3')
# Create an XGBoost classifier
xgb_classifier = xgb.XGBClassifier()
print('4')
# Train the XGBoost model on the TF-IDF transformed training data
xgb_classifier.fit(X_train_tfidf, y_train)
print('5')
# Make predictions on the TF-IDF transformed test data
y_pred = xgb_classifier.predict(X_test_tfidf)
print('6')
y_pred = le.inverse_transform(y_pred)
len(y_pred)
print('7')

final_test['Final_Assessment_Prediction'] = y_pred
# Evaluate the model
print("Classification Report:")
#print(classification_report(y_test, y_pred))

final_test.head(2)

final_test[final_test['Final Assessment CBI']==final_test['Final_Assessment_Prediction']].shape[0]/final_test.shape[0]

final_test.to_excel('./Test_Prediction_Output_Dummy_CBI.xlsx')

final_test.shape





















